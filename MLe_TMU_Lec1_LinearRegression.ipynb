{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLe_TMU_Lec1_LinearRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzUv0A4DOec7wIJInrsHqx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-H-Amini/MachineLearning-TMU/blob/master/MLe_TMU_Lec1_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXO2Fa0YBjB5",
        "colab_type": "text"
      },
      "source": [
        "# In The Name Of ALLAH\n",
        "# Machine Learning *elementary* Course\n",
        "## Tarbiat Modares University\n",
        "### Mohammad Hossein Amini (mhamini@aut.ac.ir)\n",
        "# Lecture 1 - Linear Regression\n",
        "\n",
        "<img src=\"https://github.com/M-H-Amini/MachineLearning-AUT/blob/master/stuff/MLAUT.jpg?raw=true\" width=\"400\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IljETY4yFS5a",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The theoretical stuff has been discussed in the video lectures. Let's implement a little...\n",
        "\n",
        "First of all, we should import some modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9sWGnzX1xNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOo1VsyEBkcZ",
        "colab_type": "text"
      },
      "source": [
        "# Creating Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qc9FMDVpvls",
        "colab_type": "text"
      },
      "source": [
        "Let's create a simple one dimensional dataset..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0ltTDTTAn5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.linspace(0, 50, 20)[:, np.newaxis]\n",
        "y = 15 + 1.2 * X + np.random.normal(0, 10, size=X.shape)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SYX_jQd4kT0",
        "colab_type": "text"
      },
      "source": [
        "Let's have a look at what we've created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgE-F4nk4okV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(X, y, 'rx')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqYqoHA5qDVG",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression (From Scratch!)\n",
        "Now, we implement our estimator just using **numpy**. In this method, we implement gradients calculation and weight updates (gradient descent) by hand!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMjABmyTq1I7",
        "colab_type": "text"
      },
      "source": [
        "## Hypothesis Function\n",
        "Let's implement estimator (hypothesis) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9ILUJNeqJrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def h(X, w, has_bias=False):\n",
        "  if has_bias:\n",
        "    return np.dot(X, w)\n",
        "  else:\n",
        "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "    return np.dot(X, w)\n",
        "\n",
        "w = np.array([[1], [5]])\n",
        "print(h(X, w))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r33JAUHqpuV",
        "colab_type": "text"
      },
      "source": [
        "Let's have a ```show``` function to visualize data and hypothesis easily!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMgnEb6dqw5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show(X, y, w):\n",
        "  outputs = h(X, w)\n",
        "  plt.figure()\n",
        "  plt.plot(X, y, 'rx', X, outputs, 'b--')\n",
        "  plt.show()\n",
        "\n",
        "show(X, y, w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foMO1KjbsVoo",
        "colab_type": "text"
      },
      "source": [
        "## Cost Function\n",
        "\n",
        "$ J = \\frac{1}{2m} \\sum_{i=1}^{m} (y^{(i)} - h(x^{(i)}))^2$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUohJj5bsYLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def J(X, y, w):\n",
        "  outputs = h(X, w)\n",
        "  errors = y - outputs\n",
        "  return np.dot(errors.T, errors)[0, 0] / (2 * X.shape[0])\n",
        "\n",
        "print(J(X, y, w))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWer7xyYtNEJ",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "We do the each iteration of weight updates in ```train_step```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBW2ekdhtPSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(X, y, w, lr=0.001):\n",
        "  outputs = h(X, w)\n",
        "  errors = y - outputs\n",
        "  X = np.concatenate((np.ones((X.shape[0], 1)) , X), axis=1)\n",
        "  w = w + lr * np.dot(X.T, errors) / X.shape[0]\n",
        "  return w\n",
        "\n",
        "w = np.array([[0], [0.]])\n",
        "w = train_step(X, y, w)\n",
        "print(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVKKB4ajvJq4",
        "colab_type": "text"
      },
      "source": [
        "The we do the training stuff in the ```fit``` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkV0_bRQuQhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(X, y, w0, lr=0.001, max_iters=1000, verbose=False):\n",
        "  w = w0\n",
        "  if verbose:\n",
        "    print(f'Iteration 0: J = {J(X, y, w)}')\n",
        "  for i in range(max_iters):\n",
        "    w = train_step(X, y, w, lr)\n",
        "    show(X, y, w)\n",
        "    if verbose:\n",
        "      print(f'Iteration {i + 1}: J = {J(X, y, w)}')\n",
        "  return w\n",
        "\n",
        "w = np.array([[0], [0.]])\n",
        "show(X, y, w)\n",
        "w = fit(X, y, w, max_iters=10, verbose=1)\n",
        "show(X, y, w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAdoekrHBxhi",
        "colab_type": "text"
      },
      "source": [
        "# Normal Equation\n",
        "First of all, let's implement **normal equation**. Use of normal equation, wherever possible, makes our life a lot easier! The main reason is that it is not an iterative method. You get the minimum in a few computations, instead of (maybe) thousands of iterations of gradient descent method. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLpibBqSAxd3",
        "colab_type": "text"
      },
      "source": [
        "## Finding Appropriate Weights\n",
        "By solving the normal equation, we can find the best weights for our problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7RT_0YBAwv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_weights(X, y):\n",
        "  X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "  w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
        "  return w\n",
        "\n",
        "w = find_weights(X, y)\n",
        "print(w)\n",
        "show(X, y, w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMMiyIS0Pc9I",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1kZYpzQKiV95eL6EZHL_CaE0Ca49MkLPO\" width=\"400\">\n"
      ]
    }
  ]
}